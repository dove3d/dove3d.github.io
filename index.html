<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<link href="https://fonts.googleapis.com/css2?family=Open+Sans&display=swap"
      rel="stylesheet">
<link rel="stylesheet" type="text/css" href="./resources/style.css" media="screen"/>

<html lang="en">
<head>
	<title>&#128330; DOVE: Learning Deformable 3D Objects by Watching Videos</title>
    <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/
        if you update and want to force Facebook to re-scrape. -->
	<meta property="og:image" content="Path to my teaser.jpg"/>
	<meta property="og:title" content="DOVE: Learning Deformable 3D Objects by Watching Videos." />
	<meta property="og:description" content="We propose a method for learning deformable 3D birds from videos, without keypoint, viewpoint or template shape supervision." />
    <!-- Twitter automatically scrapes this. Go to https://cards-dev.twitter.com/validator?
        if you update and want to force Twitter to re-scrape. -->
    <meta property="twitter:card"          content="summary" />
    <meta property="twitter:title"         content="DOVE: Learning Deformable 3D Objects by Watching Videos." />
    <meta property="twitter:description"   content="We propose a method for learning deformable 3D birds from videos, without keypoint, viewpoint or template shape supervision." />
    <meta property="twitter:image"         content="Path to my teaser.jpg" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Add your Google Analytics tag here -->
    <script async
            src="https://www.googletagmanager.com/gtag/js?id=UA-97476543-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());
        gtag('config', 'UA-97476543-1');
    </script>

</head>

<body>
<div class="container">
    <div class="title">
        &#128330; DOVE: Learning Deformable 3D Objects by Watching Videos
    </div>

    <div class="venue">
        arXiv preprint
    </div>

    <br><br>

    <div class="author">
        <a href="https://elliottwu.com">Shangzhe Wu</a>*
    </div>
    <div class="author">
        <a href="https://www.robots.ox.ac.uk/~tomj">Tomas Jakab</a>*
    </div>
    <div class="author">
        <a href="https://chrirupp.github.io">Christian Rupprecht</a>
    </div>
    <div class="author">
        <a href="https://www.robots.ox.ac.uk/~vedaldi">Andrea Vedaldi</a>
    </div>

    <br><br>

    <div class="affiliation">
      Visual Geometry Group, University of Oxford
    </div>

    <br><br>

    <p style="text-align: center;">( * equal contribution )</p>

    <br>

    <div class="links"><a href="">[Paper]</a></div>
    <div class="links"><a href="https://www.youtube.com/watch?v=dQw4w9WgXcQ">[Video]</a></div>
    <div class="links">[Code coming]</div>

    <br><br>

    <img style="width: 80%;" src="./resources/teaser.jpg" alt="Teaser figure."/>
    <br><br>
    <p style="width: 80%;">
        <b><i>DOVE</i></b> - Deformable Objects from VidEos. Given a collection of video clips of an object category as training data, we learn a model that is able to predict a textured, articulated 3D mesh ofthe object from a single input image.
    </p>

    <br><br>
    <hr>

    <h1>Abstract</h1>
    <p style="width: 80%;">
        Learning deformable 3D objects from 2D images is an extremely ill-posed problem. Existing methods rely on explicit supervision to establish multi-view correspondences, such as template shape models and keypoint annotations, which restricts their applicability on objects "in the wild". In this paper, we propose to use monocular videos, which naturally provide correspondences across time, allowing us to learn 3D shapes of deformable object categories without explicit keypoints or template shapes. Specifically, we present DOVE, which learns to predict 3D canonical shape, deformation, viewpoint and texture from a single 2D image of a bird, given a bird video collection as well as automatically obtained silhouettes and optical flows as training data. Our method reconstructs temporally consistent 3D shape and deformation, which allows us to animate and re-render the bird from arbitrary viewpoints from a single image.
    </p>

    <br><br>
    <hr>

    <h1>Video</h1>
    <div class="video-container">
        <iframe src="https://www.youtube.com/embed/dQw4w9WgXcQ" frameBorder="0"
                allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen></iframe>
    </div>

    <br><br>
    <hr>

    <h1>Method Overview</h1>
    <img style="width: 80%;" src="./resources/method.jpg"
         alt="Method overview figure"/>
    <br>
    <a class="links" href="https://github.com/elliottwu/webpage-template">[Code]</a>

    <br><br>
    <hr>

    <h1>Results</h1>
    <img style="width: 80%;" src="./resources/results.jpg"
         alt="Results figure"/>

    <br><br>
    <hr>

    <h1>Demo</h1>
    <div class="video-container">
        <iframe src="demo/demo.html" style="border:0;"></iframe>
    </div>
    
    <br><br>
    <hr>

    <h1>Paper</h1>
    <div class="paper-thumbnail">
        <a href="https://arxiv.org">
            <img class="layered-paper-big" width="100%" src="./resources/paper.jpg" alt="Paper thumbnail"/>
        </a>
    </div>
    <div class="paper-info">
        <h3>DOVE: Learning Deformable 3D Objects by Watching Videos</h3>
        <p>Shangzhe Wu*, Tomas Jakab*, Christian Rupprecht, Andrea Vedaldi</p>
        <p>(*equal contribution)</p>
        <p>arXiv preprint, 2021.</p>
        <pre><code>@Article{wu2021dove,
    title = {{DOVE}: Learning Deformable 3D Objects by Watching Videos},
    author = {Shangzhe Wu and Tomas Jakab and Christian Rupprecht and Andrea Vedaldi},
    journal = {arXiv preprint arXiv:xxxx.xxxxx},
    year = {2021},
}</code></pre>
    </div>

    <br><br>
    <hr>

    <h1>Acknowledgements</h1>
    <p style="width: 80%;">
        Shangzhe Wu is supported by Facebook Research. Tomas Jakab is supported by Clarendon Scholarship. Christian Rupprecht is supported by Innovate UK (project 71653) on behalf of UK Research and Innovation (UKRI) and by the European Research Council (ERC) IDIU-638009.
        This <a href="https://github.com/elliottwu/webpage-template">webpage template</a> was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a>
        and <a href="http://richzhang.github.io/">Richard Zhang</a> for a
        <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project.
    </p>

    <br><br>
</div>

</body>

</html>
